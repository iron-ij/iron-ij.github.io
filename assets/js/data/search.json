[ { "title": "머신러닝 디자인 패턴 정리 - 데이터 스케일링", "url": "/posts/machine-learning-design-patterns-2/", "categories": "머신러닝", "tags": "머신러닝, 디자인패턴", "date": "2021-12-12 21:59:00 +0900", "snippet": "아마 시중에 있는 머신러닝/딥러닝 책을 구매해서 땋! 하고 펴보면 제일 먼저 Mnist(손글씨) 데이터셋이 주어지고 이를 통해 머신러닝 문제들을 해결하는 예제를 쉽게 접해볼 수 있다.오늘 포스팅에서는 머신러닝 모델링을 진행하기 이전에 데이터 스케일링 과정은 왜 필요하며 어떤 것들이 있는지 살펴보고 책에 나와있는 내용 이외에 머신러닝 서비스를 염두할 때 고려해야할 점을 가볍게 정리해보고자 한다.#1 스케일링은 왜 필요할까? 경사하강법을 쓰는 경우에 더 빠른 수렴을 기대해볼 수 있다. loss가 크면 클수록 Computational cost가 커지고, 잘못된 가중치 업데이트로 이어질 수 있음 [-1, 1]로 스케일링을 하면 가장 높은 부동 소수점 정밀도를 얻을 수 있다. 특정 머신러닝 알고리즘에 경우, 서로 다른 Feature의 상대 크기에 매우 민감하다 유클리드 거리를 기준으로 업데이트 하는 경우, 예컨데 k-means같은 알고리즘은 상대 크기가 큰 feature에 영향을 많이 받게 된다 모델 정규화(L1, L2)의 효율성에도 영향을 준다 → 특히, 3번 이유가 가장 일반적인 이유인데, 머신러닝 알고리즘을 사용할 때는 스케일링을 항상 유념해야 한다.#2 스케일링은 어떤 방식이 있을까?2.1 Linear scaling데이터의 분포가 어느정도 일관된 형태이거나, 정규분포형의 데이터 분포를 보인다면 스케일링 진행시에 선형으로 스케일링하는 방법론을 고려해 볼 수 있다. 물론 앞서 예로든 형태를 띄지 않더라도 적용할 수 있지만 효과가 없을 수 있어서 별로 추천하지는 않는다. 선형으로 스케일링하는 경우에 자주 사용하는 방법론인데 크게 4가지가 있는데 같이 봐보자 (물론 더 있지만, 여기에선 자세하게 설명하지는 않겠다)1) Min-Max 스케일링 데이터를 최소값 -1, 최대값 1로 설정하는 스케일링이다.\\[x_{scaled} = \\frac{(2x - x_{max} - x_{min})} {x_{max} - x_{min}} , x\\in X\\] 단, min / max 고려시 아웃라이어 체크를 반드시 해야한다. 대부분 raw한 데이터를 활용하는 경우에 아웃라이어가 min / max가 되는 경우가 많으며 이런 경우 모델에 영향을 주게 된다.2) 클리핑 1차적으로 Min-Max 스케일링과 동일한 개념이지만, 전체 분포의 최대값이 아닌 특정값을 min/max로 활용한다. 예를 들어 최대값으로 특정한 값보다 큰 x의 경우에는 최대값과 동일하도록 설정한다. 특정값을 어떻게 설정해야 되는가에 물음표가 있을 수 있지만, 대부분의 경우 정답이 있지는 않다 (애도…) 그럼에도 불구하고 이런 방식을 활용하면 Min-Max 스케일링의 단점인 아웃라이어에 취약한 점에 어느정도 효과를 볼 수 있다.3)Z-score 정규화 데이터의 특성을 고려하여 정규화하는 방식으로 문제를 해결하는 방식이며, 대부분에 경우에서 좋은 성과를 발휘하고 데이터에 대한 사전 지식이 없더라도 아웃라이어를 해결 할 수 있다\\[x_{scaled} = \\frac{x-\\mu}{\\sigma}\\] 이때, $\\mu$는 x가 속한 분포의 평균이며, $\\sigma$는 x가 속한 분포의 표준편차이다.4)윈저라이징 학습 데이터의 경험적 분포를 사용하여 데이터값의 10분위 및 90분위수(or 5분위 및 95분위수 등)에 해당 하는 경계로 데이터셋을 클리핑 한다. 역시 단순 min/max 표준화에 비해서 아웃라이어를 어느정도 해결할 수 있다. 사이킷런에서는 RobustScaler를 참고해볼만 하다. RobustScaler는 기본적으로 Q1 / Q3을 기준으로 윈저라이징한다고 보면 될 것 같다.이외에는 절대값을 취한 후 Min-Max 스케일링을 진행하는 방법 등이 있다.2.2 Nonlinear scaling데이터 자체가 굉장히 치우쳐 있거나, 우리가 익히 아는 정규분포형 데이터를 띄지 않는 경우에 우선적으로 고려해 볼만한 방법이다. 간단하게 몇가지만 소개하자면 1) 로그변환(데이터 값에 로그를 취함), 2) 시그모이드 함수 적용, 3) 다항식 전개, 4) 박스콕스변환, 5) 버켓 등이 있다 개인적으로는 다항식 전개는 데이터 구조에 대한 이해가 깊지 않다면 쓰기 어려운 방법론이라 생각해서 분포에 대한 이해도가 높은 경우를 빼고는 실무에서 머신러닝 사용하면서 사용한 케이스가 많이 없다. 박스콕스 변환은 이분산성을 제어하는 하는데 효과적이다. 개인적으로는 실무에서는 로그변환, 버켓을 주로 활용한다.#3 머신러닝을 서비스에 적용할 때 고려할 부분1. Min-max 스케일러를 활용할때는 새로운 데이터에 주의하자모델을 배포해서 새롭게 데이터가 들어오는 상황을 가정해보자. 그런데 새로운 데이터가 이전에는 없던 Min-max 값을 보이고 있는 경우 모델자체가 흔들릴 수 있다. 배포 이후 들어오게 되는 데이터의 수준이 가늠되지 않는다면 클리핑 / 윈저라이저 등을 활용해서 해결하는 방법이 일반적이고, Z-score 정규화로 변경하는 것 또한 대안이 될 수 있다.2. 아웃라이어를 버리지 말자흔히 아웃라이어(이상치)로 분류되는 값들을 머신러닝 모델링에서 버리고 진행하는 케이스를 자주 보게된다. 그런데 실제로 서비스에 적용하고자 할 때는 리스크가 있을 수 있다.광고 페이지의 클릭율을 input 데이터로 활용해서 모델링을 진행한다고 가정할 때, 5%의 클릭율을 Max 클리핑하게되면 5%이상인 케이스가 발생할 때 모델이 적절하게 예측할 수 없을 수 있다.단, 잘못된 데이터는 제외하자. 사용자의 나이 데이터를 조사하는 단계에서 -5살이 입력됐다면 이런 데이터는 제외하는 것이 적절하다는 것이다.3. 버켓을 잘 활용하자휴리스틱한 부분이 들어가지만, 버켓을 잘 활용해서 데이터를 구간별로 잘나누는 것도 서비스시에 도움이 되는 방법이다. 꽤 일반적으로 사용하는 방법론이지만 거창하게 다시한번 말한 것에 불가한데, 예를 들어 나이 데이터를 활용하는 경우에 5세 단위로 구분해서 한개의 그룹핑을 하는 방법 등을 활용하면 나이만 Feature로 볼 때보다 데이터의 안정성이 증가하는 경우를 확인해볼 수 있다. (일종의 대수의 법칙이랄까…)이번 포스팅에서는 머신러닝에서 적용되는 스케일링에 종류에 대해서 다시한번 알아보고 서비스에 적용할 떄 고려할 부분에 대해서 정리해보았다. 실제 모델링 환경에서는 위 종류보다 더 어려운 상황을 만날수 있지만 대부분의 경우에는 위 케이스를 이해하고 아웃라이어에 대한 대처방법을 미리 고려하는 것만으로도 안정성을 꽤 확보할 수 있다." }, { "title": "AWS serverless로 nlp 모델 배포하기", "url": "/posts/serverless-nlp/", "categories": "nlp", "tags": "nlp, aws, ml serving", "date": "2021-12-12 21:57:00 +0900", "snippet": "AWS 배포시에 다음과 같은 장단이 있다속도(구현 속도) : AWS(lambda) - 싸다, 구현하기 쉽다(파이썬 스크립트를 짜고)그러나 아래 튜토리얼은 실제 서비스 환경과 거리가 있고 실제로는 훨씬 고려해야할 점이 많다. Aws로 이렇게 할 수 있구나! 정도만 참고하자 (아래 방식은 responce time도 긴 편이다ㅜㅜ)1. serverless 설치 AWS에서 템플릿이용해서 serverless instance를 띄우는 것을 도와주는 라이브러리 AWS → 너무 명령어가 복잡(단계가 좀 많다) ⇒ serverless를 하기위해서 좀 편하게 템플릿을 잡아주는 것(라이브러리)# mac os 기준npm install -g serverless 설치하게되면 AWS credentials 를 가져와서 기본 세팅함 없다면(AWS CLI가 세팅 안된경우) : https://www.serverless.com/framework/docs/providers/aws/guide/credentials/2. Local dir에 템플릿 생성serverless create --template aws-python3 ## --path serverless-bert (경로를 제공)# 현재 workdir에 생성# serverless create --template aws-python3# 별도 경로에 생성# serverless create --template aws-python3 --path [new_workdir_path] 실행시 handler.py lambda 실행을 위한 boilerplate code가 들어있음 serverless.yml 생성됨3. Task에 맞게 handler function 정의 code import json import torch from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast def encode(tokenizer, context): &quot;&quot;&quot;encodes the question and context with a given tokenizer&quot;&quot;&quot; encoded = tokenizer.encode(context) return encoded def decode(tokenizer, tokens): &quot;&quot;&quot;decodes the tokens to the answer with a given tokenizer&quot;&quot;&quot; return tokenizer.decode(tokens[0,:].tolist()) def serverless_pipeline(model_path=&#39;./model&#39;): MODEL_NAME = &#39;skt/kogpt2-base-v2&#39; &quot;&quot;&quot;Initializes the model and tokenzier and returns a predict function that ca be used as pipeline&quot;&quot;&quot; model = GPT2LMHeadModel.from_pretrained(MODEL_NAME) tokenizer = PreTrainedTokenizerFast.from_pretrained(MODEL_NAME, bos_token=&#39;&amp;lt;/s&amp;gt;&#39;, eos_token=&#39;&amp;lt;/s&amp;gt;&#39;, unk_token=&#39;&amp;lt;unk&amp;gt;&#39;, pad_token=&#39;&amp;lt;pad&amp;gt;&#39;, mask_token=&#39;&amp;lt;mask&amp;gt;&#39;) def gen_result(context_dict): def predict(context, max_len, pen=2.0): &quot;&quot;&quot;predicts the answer on an given question and context. Uses encode and decode method from above&quot;&quot;&quot; input_ids = tokenizer.encode(tokenizer, context) gen_ids = model.generate(torch.tensor([input_ids]), max_length=max_len, repetition_penalty=pen, pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id, bos_token_id=tokenizer.bos_token_id, use_cache=True) answer = tokenizer.decode(gen_ids) return answer predict_set = { &#39;gen_text&#39;: [predict(context_dict[&#39;main_title&#39;], max_len=90, pen=i)] } return predict_set return gen_result gpt_pipeline = serverless_pipeline() def handler(event): try: # loads the incoming event into a dictonary body = json.loads(event[&#39;body&#39;]) # uses the pipeline to predict the answer print(body) answer = gpt_pipeline(context_dict=body[&#39;context&#39;]) return { &quot;statusCode&quot;: 200, &quot;headers&quot;: { &#39;Content-Type&#39;: &#39;application/json&#39;, &#39;Access-Control-Allow-Origin&#39;: &#39;*&#39;, &quot;Access-Control-Allow-Credentials&quot;: True }, &quot;body&quot;: json.dumps({&#39;answer&#39;: answer}) } except Exception as e: print(repr(e)) return { &quot;statusCode&quot;: 500, &quot;headers&quot;: { &#39;Content-Type&#39;: &#39;application/json&#39;, &#39;Access-Control-Allow-Origin&#39;: &#39;*&#39;, &quot;Access-Control-Allow-Credentials&quot;: True }, &quot;body&quot;: json.dumps({&quot;error&quot;: repr(e)}) } # Use this code if you don&#39;t use the http event with the LAMBDA-PROXY # integration &quot;&quot;&quot; return { &quot;message&quot;: &quot;Go Serverless v1.0! Your function executed successfully!&quot;, &quot;event&quot;: event } &quot;&quot;&quot; 4. dockerbuild 및 테스트# docker builddocker build -t [docker-tag] .# docker rundocker run -p 8080:8080 [docker-tag]# check docker container using sshdocker exec -it [container_name] /bin/bash https://github.com/aws/aws-lambda-runtime-interface-emulator/ aws 에서 에뮬레이터를 제공함, 그러나 생각보다 안정적이지는 않아서 잘 쓰지는 않았음 lambda 형식에 맞는 모델 코드를 작성 이후 dockerfile로 묶기 인풋 → 추론 파이썬코드 모델파일 (gpt2) aws → dockerfile 올리기 (ECR)5. ECR 생성# 리포지토리 생성aws ecr create-repository --repository-name [docker-tag] &amp;gt; /dev/null# aws region informationaws_region=[AWS_REGION]aws_account_id=[AWS_ACCOUNT_ID]# aws ecr 로그인aws ecr get-login-password \\ --region $aws_region \\| docker login \\ --username AWS \\ --password-stdin $aws_account_id.dkr.ecr.$aws_region.amazonaws.com# Login Succeeded# docker tagdocker tag [docker-tag] $aws_account_id.dkr.ecr.$aws_region.amazonaws.com/[docker-tag]# docker pushdocker push $aws_account_id.dkr.ecr.$aws_region.amazonaws.com/[docker-tag]# ordocker push $aws_account_id.dkr.ecr.$aws_region.amazonaws.com/[docker-tag]:version6. serverless deploy aws ecr - dockerfile ecr - lambda - api gateway ← API serverless file이 있는 dir에서# work dirserverless deploy 서버리스 deploy를 사용하지 않는다면, 참고 https://blog.algopie.com/aws/aws-lambda를-이용한-api-서비스-배포-12/ http://labs.brandi.co.kr/2018/07/31/kwakjs.html7.(DEBUG) cloud watch 연동 https://aws.amazon.com/ko/premiumsupport/knowledge-center/api-gateway-cloudwatch-logs/8. 문제해결 Debug는 환경설정이 필요한데, (API gateway) API call check 은 포스트맨으로 진행 API gateway 이슈는 AWS 콘솔 설정에서 해결 코드 이슈(python &amp;amp; lambda)는 python에서 logging 남기기 8.1 파라미터 변경 그냥 body (or data)로 object를 줄때는 편했는데 request param으로 주려니 문제 발생 lambda proxy integration 하면 해결(파라미터 queryStringParameters) 하면 json으로 받음 https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-method-settings-method-request.html#api-gateway-proxy-resource 여러가지가 이슈가 있었는데 python script에서 debug 찍는게 제일 편함 (사전에 cloudwatch 연동 필요) https://sanghaklee.tistory.com/578.2 CORS 이슈 프론트에서 별도로 보내주는 CORS가 다를시에 문제 발생 CORS란? https://hannut91.github.io/blogs/infra/cors API Gateway에서 작업 → CORS 활성화 후 꼭 API를 새로 배포해야 반영됨 8.3 responce time 이슈 api gateway 최대 대기시간은 30000ms (약 30초) 따라서 응답이 30초안에 떨어져야함 이래서 경량화가 필요한가 봄8.4 (체크 필요) URL encoding issue 인코딩은 API GATEWAY 레벨에서 알아서 인코딩 해줌 (Good)다음 블로그를 reference로 참고함https://github.com/philschmid/serverless-bert-huggingface-aws-lambda-dockerhttps://aws.amazon.com/ko/blogs/machine-learning/using-container-images-to-run-pytorch-models-in-aws-lambda/서버리스 api 참고 : https://www.serverless.com/framework/docs/providers/aws/cli-reference/create/" }, { "title": "Latent Semantic Analysis (LSA, 잠재의미 분석)", "url": "/posts/latent-semantic-analysis/", "categories": "nlp", "tags": "nlp, machine learning, math", "date": "2021-12-05 21:57:00 +0900", "snippet": "잠재 의미 분석(LSA, LSI) BoW에 기반한 단어 - 문서 행렬 , TF-IDF는 기본적으로 단어의 빈도 수를 수치화하여 분석하는 방법론이었기 때문에 → 단어의 의미를 고려하지 못한다 라는 단점이 있었음 이를 보완하기 위한 대안으로 단어 - 문서 행렬의 잠재된(latent) 의미를 이끌어 내는 방법으로 잠재 의미 분석(LSA)을 활용함 (LSI 라고도 함) m x n (단어,문서) 행렬 → m x t, t x n 의 (단어, 토픽), (토픽, 문서) 행렬로 분해됨 How - to : 토픽모델링을 위해 고안된 아이디어라기 보기 보다는, SVD(개인적으로는, 활용관점에서 선대의 꽃이라고 생각)를 토픽 모델링이라는 분야에 활용한 사례라고 할 수 있음1. 특이값 분해(Singular Value Decomposition, SVD) 특이값 분해를 들어가기전에 EVD(Eigen Value Decomposition)을 잠깐 짚고 넘어가겠음 몇몇 정방행렬(n x n)은 대각화 분해 (Eigen Value Decomposition)가 가능함 n x n 행렬 A가 대각화 분해가 가능하려면, 행렬 A가 일차독립인 고유벡터(Linearly independent)를 가져야 함 → 각 벡터는 공간의 기저(Basis) 역활을 함 행렬 A를 대각화 분해하면, Det(A), A의 거듭제곱, 역행렬, 대각합(TraceA) 등을 손쉽게 계산할 수 있는 장점이 있음 정방행렬 중 H를 다음처럼 대각원소를 중심으로 원소값들이 대칭되는 행렬이라고 가정해보자 \\[H^T = H\\] 실수 원소를 갖는 대칭행렬(Symmetric matrix)은 1. 항상 고유값 대각화(EVD)가 가능하며 2. 직교행렬로 대각화가 가능함 → 이 성질로 인해 SVD, 주성분분석(PCA) 등이 가능해짐\\[H = PDP^{-1}\\]1) 일반 m x n 행렬 일반 행렬 m x n을 대각화 분해하기 위해선 어떤 과정이 필요할까? m x n 행렬 A가 있다고 가정해보자, 그러면 아래와 같이 되고\\[A^TA \\neq AA^T\\] 하나는 m x m 정방행렬, 또 하나는 n x n 정방행렬이 된다 가장 중요한점은 저 두 행렬이 모두 대칭행렬(Symmetric matrix)라는 점이다 이제 두 정방행렬은 각각 EDV가 가능한 행렬이 되었으며, 항상 직교행렬로 대각화가 될 것임2) 특이값 분해 특이값 분해(SVD)는 A가 m x n 행렬일 때, 다음과 같이 3개의 행렬 곱으로 분해하는 것을 말함\\(A=U \\sum V^T\\) 이 3개의 행렬은 다음과 같은 조건을 만족함 U = mxm 직교행렬 V = nxn 직교행렬 $\\sum$ = mxn 직사각 대각행렬 가운데 직사각 대각행렬은 각 Component(참고 SVD의 기하학적 의미 : 위키피디아 https://en.wikipedia.org/wiki/Singular_value_decomposition)3) 특이값 분해의 응용 Thin SVD SVD 행렬에서 대각행렬의 아래부분과 U에서 아래부분에 해당하는 부분을 제거 A를 완벽하게 원복할 수 있음 sparce한 부분을 모두 제거하는 것을 Compact SVD라고도 함 truncated SVD A의 SVD 대각행렬 부분에서 상위값 t개만 골라낸 형태임 A를 원복할 수 없지만, 데이터 정보를 압축했음에도 A를 근사할 수 있게되어 계산비용이 절감 반대로 생각해보면, 중요하지 않는 부분은 제외한다고 볼 수 있음 LSA에서 주로 활용하는 방식2. Latent Semantic Indexing (LSI) LSI는 k 차원의 벡터로 단어와 문서를 표현함 Topic에 관련한 정보를 보존하는 차원 압축 방법임 https://ratsgo.github.io/from frequency to semantics/2017/04/06/pcasvdlsa/ Topic Importance를 통해 토픽의 중요도 파악이 가능 또한 두 Components term 1,2의 유사성이 보존됨 두 components 텀에 doc 1,2 / doc 3, 4의 유사성이 보존됨 SVD에 대한 수학적인 리뷰는 이후에도 하겠지만, 활용사례를 정리하면서 늘상 헷갈렸던 부분을 다시한번 정리해보았고, 이제는 구현이 매우 간단하지만 아직도 비즈니스에서 효과적으로 사용할 수 있는 사례 중 하나라고 생각하며 정리글을 마무리하겠음" }, { "title": "Mac 초기환경 세팅하기", "url": "/posts/mac-default-setting/", "categories": "etc", "tags": "etc", "date": "2021-12-05 21:15:00 +0900", "snippet": " Mac 구매 또는 초기화 후에 개발환경을 세팅할 일이 간간히 있다. 2~3달에 한번정도는 하는 것 같은데, 그동안 매번 찾아서 하다가 나에게 맞는 세팅을 조금 맞춰놓고 싶어서 정리하게 되었다1. Home brew 설치$ /usr/bin/ruby -e &quot;$(curl -fsSL [https://raw.githubusercontent.com/Homebrew/install/master/install](https://raw.githubusercontent.com/Homebrew/install/master/install))&quot;$ echo &#39;export PATH=&quot;/usr/local/bin:$PATH&quot;&#39; &amp;gt;&amp;gt; ~/.bash_profile2. 기본 프로그램 설치 주된 작업환경이 vsc이다보니 우선적으로 vsc와 그에 맞는 크롬 등을 설치한다. brew cask install iterm2 google-chrome visual-studio-code brew cask install homebrew/cask-fonts/font-cascadia brew cask install homebrew/cask-fonts/font-d2coding3. Iterm2 기본 설정Iterm2 의 컬러 테마는 이 곳 에서 다운로드해서 설치 Preferences &amp;gt; Profiles &amp;gt; Colors &amp;gt; Color Presets &amp;gt; Import... 를 선택해서 다운로드 받은 컬러 테마의 압축 폴더 중에서 schemes에 있는 설정 파일을 선택 dracula 테마설치preferances Appearance &amp;gt; Theme : Dark 혹은 원하시는 Theme를 선택 Appearance &amp;gt; Windows &amp;gt; Hide scrollbars : 사용함으로 선택 Appearance &amp;gt; Windows &amp;gt; Show line under title bar when the tab bar is not visible : 사용하지 않음 Iterm 추가설정(iterm에는 기본적으로 줄삭제, 분단 삭제가 없음) command + , 로 설정 들어간 후 profile → keys command ← / commad → 지워줌 ### 왼쪽으로 단어 이동 keyboard short cut : command ← action : Send Escape Sequence esc+: b ### 오른쪽으로 단어 이동 keyboard short cut : command → action : Send Escape Sequence esc+: f ### 줄 삭제 Keyboard Shorcut: ⌘ + delete Action: Send Hex Code 0x15 ### 단어 삭제 Keyboard Shorcut: ⌥ + delete Action: Send Hex Code 0x17 4. ZSH 설정 iterm2에서 ZSH를 설치(brew install zsh) oh-my-zsh을 설치(sh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&quot;) zsh 플러그인을 설치 brew install zsh-completions fasdgit clone https://github.com/zsh-users/zsh-syntax-highlighting.git $ZSH_CUSTOM/plugins/zsh-syntax-highlightinggit clone git://github.com/zsh-users/zsh-autosuggestions $ZSH_CUSTOM/plugins/zsh-autosuggestions 설치된 플러그인을 ZSH에서 활성화 하기 위해서 ~/.zshrc 파일의 plugins 항목에 git, zsh-syntax-highlighting, zsh-autosuggestions, fasd 를 설치 주의 ,(Comma) 없이 넣어주면 됨 5. Python 설치 주로 사용하는 환경이 Python 이기 때문에 이에 맞는 설치를 진행brew install pyenv$ code .zshrc// pyenv를 추가하세요.plugins=( git zsh-syntax-highlighting zsh-autosuggestions fasd pyenv)pyenv install 3.7.6pyenv rehashpyenv global 3.7.66. Notion, 1password 설치 이부분은 개인 취향 및 환경이 반영된 부분이기 때문에 참고만 해도 될 듯하다.이렇게 내가 기본적으로 쓰는 환경을 정리해보았다.시간이 지남에 따라 사용하는 부분도 많아질 테니 중간중간 업데이트할 예정이다." }, { "title": "2.5 Linear independence", "url": "/posts/linear-independence/", "categories": "Math", "tags": "linear equation, mml-book", "date": "2021-11-28 21:50:00 +0900", "snippet": " 이번장에서 알고 가야할 정의 Linear Combination Linear Independent vector Linear dependent vector Definition 2.11 (Linear Combination) 벡터 공간 $V$와 유한한 벡터 $x_1,…,x_k \\in V$ 이면, 모든 $v \\in V$이 다음을 만족하면 벡터 $x_1,…,x_k \\in V$와 $\\lambda_1,…,\\lambda_k \\in R$의 선형결합(Linear combination)이라 정의한다 0-벡터는 항상 선형결합이다.\\[v=\\lambda_1x_1 + \\cdot\\cdot\\cdot+\\lambda_kx_k=\\sum^k_{i=1}\\lambda_ix_i \\in V\\]Definition 2.12 (Linear (In)dependence)벡터 공간 $V$와 유한한 벡터 $x_1,…,x_k \\in V$, $k \\in N$ 이 존재하면, $\\lambda_i \\ne 0$이 아닌 해가 존재하면 선형 종속(linear dependence)이라 정의함 $\\lambda_i = 0$ 만 해를 만족한다면 선형 독립(linear independence)이라 정의함 모든 컬럼 벡터가 pivot 이면 선형 독립의 필요충분(if and if only) 조건임 → 모든 컬럼이 pivot 인 것을 보이면 선형 독립(linear independence)을 보일 수 있음 {$x_1,…,x_m$}이 선형 독립(linear independence)이면, 컬럼 벡터 {$\\lambda_1,…,\\lambda_m$} 역시 선형 독립임 {$x_1,…,x_m$}이 선형 독립(linear independence)이더라도, 컬럼 벡터 {$\\lambda_1,…,\\lambda_n$} 은 선형 종속 일 수 있음 ($n &amp;lt; m$) 이것이 왜 중요한가? → 선형 시스템의 해를 구하는 것은 결국 주어진 벡터들의 선형결합을 찾는 것과 같음 선형 독립의 부분집합은 선형독립이다." }, { "title": "2.4 Vector space", "url": "/posts/vector-space/", "categories": "Math", "tags": "linear equation, mml-book", "date": "2021-11-25 22:05:00 +0900", "snippet": " 이번장에서 알고 가야할 정의 Group (군) Vector space Vector subspace 한창 linear system에 대해서 소개하다가 왜 Vector space가 등장할까? 라고 생각을 해보면 이후에 Vector space에 대한 정의를 하기위해서 밑밥(?)을 다진다고 보면 될 것 같다.2.4.1 GroupDefinition 2.7 (Group) 어떤 집합 $\\mathcal{G}$와 연산 $\\oplus$가 다음과 같이 정의되면, $\\oplus$ : $\\mathcal{G}$X $\\mathcal{G}$ → $\\mathcal{G}$,$G$ := ($\\mathcal{G}$,$\\oplus$) 은 다음을 따를 때 정의됨 연산 $\\oplus$ 에 대해 닫혀있음(Closure) $\\forall$$x, y \\in \\mathcal{G} \\ : \\ x \\oplus y \\in \\mathcal{G}$ 결합법칙이 성립함(Associativity) → $\\forall x, y, z \\in \\mathcal{G} \\ : \\ (x\\oplus y )\\oplus z = x \\oplus(y \\oplus z)$ 항등원이 존재함(neutral element) → $\\exists e \\in \\mathcal{G} \\ \\forall x \\in \\mathcal{G} \\ : \\ x \\oplus y = x \\ \\ and \\ e \\oplus x = x$ 역원이 존재함(Inverse element) → $\\forall x \\in \\mathcal{G} \\ \\exists y \\in \\mathcal{G} \\ : \\ x \\oplus y = e \\ and \\ y \\oplus x = e$ 이때, $x$의 역원을 $x^{-1}$로 표기 (Abelian Group) 교환법칙이 성립하면 아벨리안 그룹임 → $\\forall x, y \\in \\mathcal{G} \\ : \\ x \\oplus y = y \\oplus x$ 1~4번을 만족하고 5번 또한 만족하는 경우 $G = ($$\\mathcal{G}$,$\\oplus )$ is Abelian group Example 2.10 (Group) 그룹의 성질과 관련한 몇가지 집어 볼 사실들 $(Z,+)$는 그룹임 $(N_{0},+)$는 그룹이 아니다 ⇒ 역원이 포함되지 않기 때문에 $(Z,\\cdot)$에 대해서 그룹이 아니다 ⇒ 항등원인 1은 포함하지만, $\\pm1$을 제외하면 역원이 없음! $(R,\\cdot)$에 대해서 그룹이 아니다 ⇒ 0이 역원을 가지고 있지 않기 때문에! $(R ,\\cdot)$에서 0을 제외하면 아벨리안 그룹이다. $(x_1,…,x_n) + (y_1,…,y_n) = (x_1 + y_1,…,x_n+y_n)$이 성립하면 $(R^n ,+), \\ (Z^n ,+), \\ n \\in N$ 에서 아벨리안 그룹이다. 이때, $(-x_1,…,-x_n)$이 역원이고, $e = (0,…,0)$ 이 항등원임 $(R^{n \\times n}, \\cdot)$ 에 대해서는 다음을 고려해 볼 수 있음 Closure &amp;amp; Associativity 는 행렬곱의 정의에 따라 성립함 항등원은 $I_n$이 존재함 $A \\in R^{n \\times n}$에 대해 $A$가 regular라면 역원이 성립하고, 이때 General linear group 이라 부름 Definition 2.8 (General linear group) 역행렬이 존재하는 $A \\in R^{n \\times n}$ 의 집합은 General linear group 이라하며, $GL(n, R)$로 표기함 그러나 교환법칙이 성립하지 않기에 아벨리안 그룹은 아님Definition 2.9 (Vector space) 벡터 스페이스 $V = (\\mathcal{V}, +, \\cdot)$ 는 집합 $\\mathcal{V}$와 2개의 연산으로 구성됨\\[+ : \\mathcal{V} \\times \\mathcal{V} Rightarrow \\mathcal{V} \\\\ \\cdot : R \\times \\mathcal{V} Rightarrow \\mathcal{V}\\] $(\\mathcal{V}, +)$ 은 아벨리안 그룹임 분배법칙(Distributivity) 이 성립 $\\forall \\lambda \\in R, x,y\\in \\mathcal{V} \\ : \\ \\lambda \\cdot(x + y) = \\lambda \\cdot x + \\lambda \\cdot y$ $\\forall \\lambda, \\psi \\in R, x \\in \\mathcal{V} \\ : \\ (\\lambda + \\psi)\\cdot x = \\lambda \\cdot x + \\psi \\cdot x$ 결합법칙(Associdativity)이 성립함 : $\\forall \\lambda,\\psi \\in R, x \\in \\mathcal{V} \\ : \\ \\lambda \\cdot (\\psi \\cdot \\ x) = (\\lambda \\psi)\\cdot x$ 상수의 행렬곱에 대해 항등원이 존재함 : $\\forall x \\in \\mathcal{V}, \\ : \\ 1\\cdot x = x$ 여기에서 행렬의 elementwise 곱연산은 고려하지 않으며, 행렬곱은 외적(outer product) &amp;amp; 내적(inner/scalar/dot product)으로 구분함Definition 2.10 (Vector Subspace) $V = (\\mathcal{V}, +, \\cdot)$ 이 벡터 스페이스이면서 $\\mathcal{U} \\subseteq \\mathcal{V}, \\mathcal{U} \\ne \\emptyset$ 을 만족하면, $U = (\\mathcal{U}, +, \\cdot)$ 는 $\\mathcal{V}$의 벡터 서브스페이스(or linear subspace) 라 정의함 $U$ 또한 벡터 스페이스의 vector subspace 이므로, 벡터스페이스의 정의를 만족함오늘은 간단하게 벡터 공간에 대해 정의를 집어보았다. 이후에 이어질 개념을 이해하기 위한 밑밥정도이니 간단하게 음미하고 지나가면 될 것 같다." }, { "title": "머신러닝 디자인 패턴 정리(1)", "url": "/posts/machine-learning-design-patterns-1/", "categories": "머신러닝", "tags": "머신러닝, 디자인패턴", "date": "2021-11-21 16:10:00 +0900", "snippet": "시작하며 하염없이 서점을 돌아다니다가 제목이 느낌있는 책을 발견했다. 보통 책을 구매할 때도, 제목과 목차만보고 구매하는 편인데 머신러닝 디자인 패턴 이라는 제목에 끌렸고, 개발 서적에서나 보던 머신러닝 코드를 짤 때 적용가능한 디자인 패턴인가? 싶어서 집어들어 보았는데 정작 그런 상상과는 거리가 있는 내용이었다. 이 책에서는 알고리즘에 대한 설명, 구성 요소, 머신러닝 아키텍처, 계층(layer)등을 기본적으로 다루지 않고 있고, 실무에서 머신러닝 시스템을 구현/운영하고자 할 때, 사용할만한 고려요소를 담았다고 보면 될 것 같다.1) 누구를 위한 책인가? 이미 머신러닝에 대해서 어느정도 지식을 가지고 계신 분 머신러닝을 할때 마주하는 상황에 대한 실용적인 대안이 필요하신 분 머신러닝을 실무로 사용하고 있으며, 현재나 향후에 머신러닝 시스템을 구축하는 방법이 필요하신 분2) 어떤 방향을 정리를 할 것인가? 책 자체에 대한 내용을 모두 정리하지는 않을 예정이다 내가 유사한 상황에 놓였을 때 사용할 수 있는 패턴을 정리해두고 꺼내쓰기 위한 용도이다Chap1. 머신러닝 디자인 패턴의 필요성1.1 머신러닝 디자인 패턴이란? 디자인 패턴은 일반적으로 발생하는 문제에 대한 모범 사례와 솔루션을 추려낸 패턴을 가리키는 공학 용어를 말한다 각 패턴은 반복해서 발생하는 문제와 그 문제 대한 솔루션의 핵심을 설명하고 있고, 그 패턴을 활용하면 같은 방식을 두 번 고안하지 않고서도 큰 틀안에서 해결할 수 있다 물론 디자인 패턴은 추상적이기 때문에 같은 솔루션이더라도 각자의 선호도, 조건과 상황에 따라 응용할 수 있음 머신러닝이 주류가 됨에 따라서 비즈니스 문제에 머신러닝을 적용하는 실무자는 반복되는 문제를 해결하기 위해 검증된 방법을 활용해야 한다.1.2 머신러닝 디자인 패턴이 필요한 이유? ML 시스템을 구축하는 프로세스 내에서는 ML 설계에 영향을 미치는 다양한 문제가 존재하므로, 이러한 문제를 패턴화 해두면 ML 실무자는 사고 방향에 대한 프레임을 설계할 수 있게 된다. 이러한 문제는 머신러닝 전체 프로세스에 얽혀있는 다양한 문제로 세분화하고 확장해서 고민해볼 수 있다. 특히 실무에서 발생할 수 있는 상황은 다음과 같은 부분을 고려해야 한다 데이터 품질 정확도: 데이터 자체에 신뢰성이 있어야 함 완전성: 주어진 문제에 대해 학습 데이터에 다양한 표현이 고려되도록 해야 함 일관성: 라벨링과 같은 작업을 할 때, 일관된 기준으로 진행해야 함 적시성: 데이터의 사건이 발생한 시점과 저장된 시점의 지연시간 재현성 설계한 모델의 결과가 일관되어야 한다. 대부분 머신러닝 프로젝트에서는 랜덤시드를 사용하고 있고, 프레임워크/라이브러리 등을 표준화하고 컨테이너에서 ML 워크로드를 실행하는 것이 좋음 데이터 드리프트 데이터의 성격이 시간 흐름에 따라 크게 바뀔 수 있고 이에 따라 모델의 성능이 하락할 수 있음 이를 해결하기 위해서는 지속적으로 데이터셋을 업데이트하고, 모델을 재학습 해야한다. 확장 실제로 서빙을 목표로하는 머신러닝 모델이라면 데이터셋/모델이 연구단계와 서빙단계에서 차이가 있을 수 있으며, 인프라나 내부 리소스등 상황에 따라 고려사항이 달라진다 목표 모델의 목표를 정의할때 조직 내 각 팀의 요구사항이 모델과 어떻게 관련되는지 고려하는 것이 중요함 모든 책이 그러하듯 첫 챕터는 전체 책에 대한 개략적인 이야기를 다루고 있다. 조금은 학술적인 내용인 것 같지만, 이후 챕터보다는 조금 더 데이터를 다루는 쪽에 가까운 것 같고, 이미 알고있거나 써본적있지만 정리해볼만한 내용들이 있었다. 이후에는 Chap2~8까지의 내용을 챕터별로 정리해볼 예정이다." }, { "title": "2.3 Solving system of linear equations", "url": "/posts/solving-system-of-linear-equations/", "categories": "Math", "tags": "linear equation, mml-book", "date": "2021-11-15 21:30:00 +0900", "snippet": "2.3 Solving Systems of Linear Equations 이번 장에서는 linear equations의 일반적인 해와 역행렬을 구하는 알고리즘에 대해서 알아보자2.3.1 Particular and General Solution 다음 선형시스템이 주어져있다고 가정해보자.\\[\\begin{bmatrix} 1 &amp;amp; 0 &amp;amp; 8 &amp;amp; -4 \\\\ 0 &amp;amp; 1 &amp;amp; 2 &amp;amp; 12 \\end{bmatrix}\\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ x_{3} \\\\ x_{4} \\end{bmatrix}=\\begin{bmatrix} 42 \\\\ 8 \\end{bmatrix}\\] 위의 식은 두개의 선형방정식과 4개의 미지수가 주어져 있음 ⇒ 이런 경우 일반적으로 무수히 많은 해가 존재 $\\sum^{4} _{i=1} x _{i} c _{i} =b$를 만족하는 $x_i$의 형태를 찾아야 함 가장 먼저 가정해볼 수 있는 것은 첫번째와 두번째 컬럼의 조합으로 $b$를 만족하는 케이스이며, 다음과 같음\\[b=\\begin{bmatrix} 42 \\\\ 8 \\end{bmatrix}=42\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}+8\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\] $\\left[ 42,8,0,0\\right] ^{T}$가 제시된 선형시스템을 만족하며, particular solution or special solution이라 불림 그러나 위의 해는 유일해는 아님 기존에 해에서 부터 0을 더해가며 special solution이 변화하지 않는 다른 해를 찾을 수 있음 ⇒ 선형시스템의 3번째 컬럼을 예를 들어보면…\\[\\begin{bmatrix} 8 \\\\ 2 \\end{bmatrix}=8\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}+2\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\] 위의 컬럼값을 0으로 만드는 $x_3$의 값을 찾아보자\\[\\begin{bmatrix} 1 &amp;amp; 0 &amp;amp; 8 &amp;amp; -4 \\\\ 0 &amp;amp; 1 &amp;amp; 2 &amp;amp; 12 \\end{bmatrix}\\left( \\lambda _{1}\\begin{bmatrix} 8 \\\\ 2 \\\\ -1 \\\\ 0 \\end{bmatrix}\\right) = \\lambda _{1}\\left( 8c_{1}+2c_{2}-c_{3}\\right) =0\\] 마찬가지로 $x_4$에도 적용해보면…\\[\\begin{bmatrix} 1 &amp;amp; 0 &amp;amp; 8 &amp;amp; -4 \\\\ 0 &amp;amp; 1 &amp;amp; 2 &amp;amp; 12 \\end{bmatrix}\\left( \\lambda_{2}\\begin{bmatrix} -4 \\\\ 12 \\\\ 0 \\\\ -1 \\end{bmatrix}\\right) =\\lambda _{2}(-4c_{1}+12c_2-c_4)=0\\] 최종 일반해(general solution)는 다음과 같음\\[\\{ x\\in \\mathbb{R} ^{4}:x=\\begin{bmatrix} 42 \\\\ 8 \\\\ 0 \\\\ 0 \\end{bmatrix}+\\lambda_1\\begin{bmatrix} 8 \\\\ 2 \\\\ -1 \\\\ 0 \\end{bmatrix}+\\lambda _{2}\\begin{bmatrix} -4 \\\\ 12 \\\\ 0 \\\\ -1 \\end{bmatrix},\\lambda _{1},\\lambda _{2}\\in \\mathbb{R} \\}\\]Remark 위의 해를 도출하는 과정은 다음과 같은 3가지 스텝으로 진행되었음 $Ax=b$를 만족하는 해를 도출 $Ax=0$을 만족하는 모든 해를 구함 1 + 2를 혼합하여 최종 일반해를 도출함 2.3.2 Elementary Tranformations기본적인 행렬변환을 통해 해를 도출할때는 다음과 같은 단순 과정의 반복으로 해결이 가능함 두 행의 위치를 바꿈 (선형시스템의 형태로 표현된 row) row에 $\\lambda \\in \\mathbb{R} \\backslash {0}$를 곱함 두 행을 더함Remark 행에서 첫번째로 0이 아닌 coefficient를 $pivot$이라 함 $pivot$이 오른쪽에 위치할 수록 아래 행으로 배치가 되어야함 그러므로, row-echelon form 의 행렬은 계단식(“staircase”) 구조를 가짐Definition 2.6(Row-Echelon Form).⇒ 다음을 만족하는 matrix는 Row-Echelon Form이라 정의함 0 만 존재하는 행이라면, matrix의 가장 아래쪽에 위치함 ⇒ 행에 0이 아닌 값이 존재한다면, 0만 존재하는 행보다 위에 위치함 행에서 처음으로 0이 아닌 값이 먼저 등장(=$pivot$)할 수록 matrix의 위쪽에 위치해야함 Remark (Basic and Free Variables) Row-Echelon Form 의 $pivot$은 $basic \\ variables$ 로 정의됨 다른 값들은 $free \\ variables$라 불림Remark (Obtaining a Particular Solution) Row-Echelon Form 의 행렬은 특정해를 정의하기 편리한 특징이 있음Remark (Reduced Row Echelon Form)다음을 만족하는 선형시스템은 Reduced Row-Echelon Form 이라 함 Row-Echelon Form의 선형시스템임 모든 $pivot$이 1임 $pivot$이 각 열에서 0이 아닌 유일한 값임Remark (Gaussian Elimination) 가우시간 엘리미네이션은 선형변환을 통해 선형시스템을 reduced row-echelon form으로 변환하는 알고리즘임 다음 행렬은 reduced row-echelon form을 따르고 있음\\[A=\\begin{bmatrix} 1 &amp;amp; 3 &amp;amp; 0 &amp;amp; 0 &amp;amp; 3 \\\\ 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 9 \\\\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; -4 \\end{bmatrix}\\] 위의 선형시스템의 일반해를 도출하는 키 아이디어는 $pivot$이 없는 컬럼으로 부터 $Ax = 0$ 를 만족하도록 하는 것! 2번째 컬럼은 1번째 컬럼의 3배수로 표현\\[\\begin{bmatrix} 3 \\\\ 0 \\\\ 0 \\end{bmatrix}=3\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}\\] 5번째 열은 31번째 열 + 93번째 열 + -4*4번째열과 동일\\[\\begin{bmatrix} 3 \\\\ 9 \\\\ -4 \\end{bmatrix}=3\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}+9\\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}-4\\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\\] 따라서 다음과 같이 나타낼 수 있음\\[\\{ x\\in \\mathbb{R} ^{5}:x=\\lambda _{1}\\begin{bmatrix} 3 \\\\ -1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}+\\lambda _{2}\\begin{bmatrix} 3 \\\\ 0 \\\\ 9 \\\\ -4 \\\\ -1 \\end{bmatrix},\\lambda_{1},\\lambda_{2}\\in \\mathbb{R} \\}\\]2.3.3 The Minus-1 Trick $Ax=0$을 풀기위한 트릭을 소개하고자함 $A$를 reduced row-ehcelon form 이라 가정하면 다음과 같은 형태를 띔 는 실수이며, pivot 이 1인 열은 이외에는 모두 0을 만족한다(rref) 이때 $pivot$ 아래에 $\\left[ 0 \\ldots 0 \\ -1 \\ 0\\ldots 0\\right]$ 을 추가하여 변형을 가해줌Inverse matrix 정의로부터 $AX=I_{n}$ 이면, $X=A^{-1}$ 이고, 선형시스템의 형태를 변형해가면서 역행렬을 구할 수 있음 $[ A \\vert I_{n}]$ ⇒ $[ I_{n} \\vert A^{-1}]$2.3.4 Algorithms for Solving a System of Linear Equations 우선적으로 $Ax = b$ 형태의 선형 시스템을 푸는 방법에 대해서 알아보고자 한다. (해가 존재한다고 가정한다, 해가 없는 케이스는 Chapter9 에서 다룰 예정이다) 특별한 경우에는 역행렬 $A^{-1}$이 정의되기 때문에 $Ax=b$ ⇒ $x = A^{-1}b$ 의 형태로 변형하여 해결할 수 있다. 그러나 위 케이스는 $A$가 정방행렬이고 역행렬이 존재할 때 가능하다. 그러므로, 약한 가정 아래($A$가 선형 독립인 열을 가져야 함)에 다음과 같은 변형을 진행해 볼 수 있다.\\[Ax=b\\Leftrightarrow A^{T}Ax \\Leftrightarrow A^{T}b \\Leftrightarrow x = (A^{T}A)^{-1}A^{T}b\\] Moore-Penrose pseudo-inverse $(A^{T}A)^{-1}A^{T}$ 를 이용하여 $Ax = b$를 풀 수 있다. 그러나 위 과정은 매우 많은 행렬 곱 연산과 역행렬을 구하는 과정이 포함되어 있기 때문에 계산비용이 많이 들고 더구나 역함수 or pseudo-역행렬을 계산하기 위해 부동소수점 연산과정이 많이 포함되는 것은 일반적으로 추천하지 않는다 따라서, 선형시스템을 풀기 위해 조금 더 간단한 접근법을 논의 해보고자 함 Gaussian elimination 은 Determinants를 구하기 위해 중요함 각 벡터가 선형 독립인지 체크하는 과정이 필요함 행렬의 역행렬을 계산함 행렬의 rank를 계산함 행렬의 basis를 계산 대부분의 선형시스템이 바로 해를 구할 수 없기 때문에 다양한 접근법이 존재함 리차드슨 메소드 자코비 메소드 가우스 사이델 메소드 가령 $x_*$가 $Ax=b$의 해 일때, 반복적인 방법을 통해 해를 도출함\\[x^{(k+1)} = Cx^{(k)} + d\\] 적당한 $C$와 d를 활용해서 오차를 점차 줄일 수 있으며, 이후에 조금더 자세히 다룰 예정이다" }, { "title": "2.1 System of Linear equation", "url": "/posts/system-of-linear-equation/", "categories": "Math", "tags": "linear equation, mml-book", "date": "2021-10-12 23:54:00 +0900", "snippet": "2.1 System of Linear Equation 일상 생활속의 많은 문제들이 선형 방정식(linear equation)으로 정의되며, 선형대수학은 이것을 풀 수 있는 방법을 제시하는 학문임 실생황을 연결지어보면 일상의 많은 문제들이 선형 방정식으 ㅣ일부라는 것을 알 수 있다, 아래와 같은 문제 역시 선형방정식의 한 예이다Ex1.가령 내가 투자할 수 있는 돈의 총액이 10만원 일때, 7만원짜리 A주식과 1만원짜리 B주식 5천원 짜리 C주식이 있다면 어떤 비율로 주식을 살 것인가? 기억속 어딘가에서 배웠던 선형방정식의 해에 대한 조건을 꺼내보자 해가 존재하지 않는 케이스 \\[x_1 + x_2 + x_3 = 3 \\\\ x_1 - x_2 + 2x_3 = 2 \\\\ 2x_1 + 3x_3 = 1\\] 유일한 해가 존재\\[x_1 + x_2 + x_3 = 3 \\\\ x_1 -x_2 +2x_3 = 2 \\\\ x_2 + x_3 = 2\\] 무수히 많은 해가 존재\\[x_1 + x_2+x_3 = 3 \\\\ x_1 - x_2 +2x_3 = 2 \\\\ 2x_1 + 3x_3 = 5\\]Remark (Geometric Interpretation of Systems of Linear Equations) 2개의 변수($x_1, x_2)$를 가지는 2개의 선형방정식은 각각 $x_1x_2-plane$의 정의됨\\[4x_1 + 4x_2 = 5\\\\2x_1 - 4x_2 = 1\\]위 식의 해는 $(x_1, x_2) = (1, 1/4)$ 임 다음 노테이션은 동치임\\[x_1\\begin{bmatrix} a_{11} \\\\ \\vdots \\\\ a_{m1} \\end{bmatrix}+x_{2}\\begin{bmatrix} a_{12} \\\\ \\vdots \\\\ a_{m2} \\end{bmatrix}+\\ldots +x_{n}\\begin{bmatrix} a_{1n} \\\\ \\vdots \\\\ a_{mn} \\end{bmatrix}=\\begin{bmatrix} b_1, \\\\ \\vdots \\\\ b_{n} \\end{bmatrix}\\]2.2 Matrices 선형 대수의 기본적인 표현인 매트릭스에 대해 알아보자Definition 2.1 (Matrix) with $m,n\\in \\mathbb{N}$ a real-valued $(m,n)$ matrix A is an m,n-tuple of elements $a_{i,j}$,i = 1,…,m j = 1,…,n which is ordered according to a rectangular scheme consisting of m rows and n columns:\\[A=\\begin{bmatrix} a_{11} &amp;amp; a_{21} &amp;amp; \\ldots &amp;amp; a_{1n} \\\\ a_{21} &amp;amp; a_{22} &amp;amp; \\ldots &amp;amp; a_{2n} \\\\ \\vdots &amp;amp; \\vdots &amp;amp; &amp;amp; \\vdots \\\\ a_{m1}, &amp;amp; a_{m2} &amp;amp; \\ldots &amp;amp; a_{mn} \\end{bmatrix}, \\ a_{ij}\\in \\mathbb{R}\\]by convention (1,n)-matrixces are called rows and (m,1)-matrices are called columns. these special matrices are also called row/column vectors $\\mathbb{R} ^{m\\times n}$ 은 실수원소의 (m,n) 매트릭스를 의미함 $A\\in \\mathbb{R} ^{m\\times n}, \\ a\\in \\mathbb{R} ^{mn}$ 은 동일한 표현임2.2.1 Matrix Addition and Multiplication 덧셈은 element-wise sum! (즉, 항끼리 더해진다는 의미임)\\[A+B:=\\begin{bmatrix} a_{11}+b_{11}, &amp;amp; \\ldots a_{1n}+b_{1n} \\\\ \\vdots &amp;amp; \\vdots \\\\ a_{m1},+b_{m1}, &amp;amp; \\ldots a_{mn}+b_{mn} \\end{bmatrix}\\in \\mathbb{R} ^{m\\times n}\\] 곱셈은 다르게 정의됨 (element-wise sum이 아님!) \\[c_{ij}=\\sum ^{n}_{l=1}a_{il}b_{lj} \\\\ i=1,\\ldots ,m, \\ j=1,\\ldots ,k\\] 다른 단원에서 내적(dot-product)을 배울텐데 이 경우 명시적으로 dot을 표기함 행렬 곱에서 두 벡터의 demension이 다르면, 곱셈한 후에 AB &amp;amp; BA의 사이즈가 다름 프로그래밍에서 사용되는 element-wise 곱은 Hadamard product라고도 불림Definition 2.2 (Identity Matrix). In $\\mathbb{R}^{n\\times n}$, we define the identity matrixas the n x n-matrix containing 1 on the diagonal and 0 everywhere else.\\[I_{n}:=\\begin{bmatrix} 1 &amp;amp; 0 &amp;amp; \\ldots &amp;amp; 0 \\\\ 0 &amp;amp; 1 &amp;amp; \\ldots &amp;amp; 0 \\\\ \\vdots &amp;amp; \\vdots &amp;amp; \\ddots &amp;amp; \\vdots \\\\ 0 &amp;amp; 0 &amp;amp; \\ldots &amp;amp; 1 \\end{bmatrix}\\in \\mathbb{R} ^{n\\times n}\\] Associativity(결합법칙):\\[\\begin{aligned}\\forall A\\in \\mathbb{R} ^{m\\times n},B\\in \\mathbb{R} ^{m\\times p},C\\in \\mathbb{R} ^{p\\times q}\\ :\\left( AB\\right) C=C\\left( AB\\right) \\end{aligned}\\] Distributivity(분배법칙):\\[\\begin{aligned}\\forall A,B\\in \\mathbb{R} ^{m\\times n},C,D\\in \\mathbb{R} ^{n\\times p},\\\\ \\left( A+B\\right) C=AC+BC\\\\ A\\left( C+D\\right) =AC+AD\\end{aligned}\\] Multiplication with the identity matrix:\\[\\forall A\\in \\mathbb{R} ^{m\\times n} \\ : \\ I_{m}A=AI_{m}=A\\] note that $I_{m}\\neq I_{n}$ for $m \\neq n$.2.2.2 Inverse and TransposeDefinition 2.3 (Inverse). Consider a square matrix $A\\in \\mathbb{R} ^{n\\times n}$ . Let matrix $B\\in \\mathbb{R} ^{n\\times n}$have the property that $AB=I_{n}=BA$. $B$ is Called the inverse of $A$ and denoted by $A^{-1}$ 모든 행렬들이 역행렬(Inverse matrix)를 가지지 않으며, 역행렬이 존재하면 regular / Invertible / nonsingular 라고 부름 (반대로) singular / noninvertible. 역행렬\\[\\begin{aligned} A^{-1}=\\dfrac{1}{a_{11}a_{22}-a_{12}a_{21}}\\begin{bmatrix} a_{22} &amp;amp;&amp;amp; -a_{12} \\\\ -a_{21}&amp;amp;&amp;amp; a_{11} \\end{bmatrix}\\end{aligned}\\] $a_{1},a_{22}-a_{12}a_{21} \\neq 0$ 인 조건에서 2 by 2 matrix의 역행렬이 성립함Definition 2.4 (Transpose Matrix). for $A\\in \\mathbb{R} ^{m\\times n}$ the matrix $B\\in \\mathbb{R} ^{n\\times m}$ with $a_{ij} = b_{ji}$ is called transpose of A. we write $B = A^{T}$ invertible &amp;amp; transpose 한 matrix는 다음 성질을 가짐\\[\\begin{aligned}AA^{-1}=I=A^{-1}A\\\\ \\left( AB\\right) ^{-1}=B^{-1}A^{-1}\\\\ \\left( A+B\\right) ^{-1}=A^{-1}+B^{-1}\\\\ \\left( A^{T}\\right) ^{T}=A\\\\ \\left( A+B\\right) ^{T}=A^{T}+B^{T} \\\\ \\left( AB\\right) ^{T}=B^{T}A^{T}\\end{aligned}\\]Definition 2.5 (Symmetric Matrix). A matrix $A$ is symmetric if $A = A^{T}$ Symmetric 한 두 matrix 의 합은 symmetric이지만, 두 matrix의 곱은 반드시 Symmetric 하지는 않음\\[\\begin{bmatrix} 1 &amp;amp; 0 \\\\ 0 &amp;amp; 0 \\end{bmatrix}\\begin{bmatrix} 1 &amp;amp; 1 \\\\ 1 &amp;amp; 1 \\end{bmatrix}=\\begin{bmatrix} 1 &amp;amp; 1 \\\\ 0 &amp;amp; 0 \\end{bmatrix}\\]2.2.3 Multiplication by a Scalar 상수와 Matrix의 곱에는 다음과 같은 성질이 존재함 let $A\\in \\mathbb{R} ^{m\\times n}$ and $\\lambda \\in \\mathbb{R}$. Then $\\lambda A = K$, $K_{ij} = \\lambda a_{ij}$. $\\lambda$ scales each elememt of $A$. for $\\lambda ,\\psi \\in \\mathbb{R}$ 이라고 하면 다음을 만족함 Associativity\\[\\left( \\lambda \\psi \\right) C=\\lambda \\left( \\psi C\\right) ,C\\in \\mathbb{R}^{m\\times n} \\\\ \\begin{aligned}\\lambda \\left( BC\\right) =\\left(\\lambda B\\right) C=B\\left(\\lambda C\\right) =\\left( BC\\right) \\lambda \\\\ B\\in \\mathbb{R} ^{m\\times n},C\\in \\mathbb{R} ^{n\\times k}\\end{aligned} \\\\ \\left( \\lambda C\\right) ^{T}=C^{T}\\lambda ^{T}=C^{T}\\lambda =\\lambda C^{T}\\]since $\\lambda = \\lambda^{T}$ for all $\\lambda \\in \\mathbb{R}$ Distributivity\\[\\begin{aligned}\\left( \\lambda +\\psi \\right) c=\\lambda C+\\psi C\\\\ C\\in \\mathbb{R} ^{m\\times n}\\end{aligned}\\]\\[\\begin{aligned}\\lambda \\left( B+C\\right) =\\lambda B+\\lambda C\\\\ B,C\\in \\mathbb{R}^{m\\times n} \\end{aligned}\\]2.2.4 Compact Representations of Systems of Linear Equations 다음 선형방정식을 아래와 같이 간결하게 표현할 수 있음\\[2x_1 + 3x_2 + 5x_3 = 1 \\\\ 4x_1 - 2x_2 - 7x_3 = 8 \\\\ 9x_1 + 5x_2 - 3x_3 = 2\\]\\[\\begin{aligned} \\begin{bmatrix} 2 &amp;amp; 3 &amp;amp; 5 \\\\ 4 &amp;amp; -2 &amp;amp; -7 \\\\ 9 &amp;amp; 5 &amp;amp; -3 \\end{bmatrix}\\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ x_{3} \\end{bmatrix}=\\begin{bmatrix} 1 \\\\ 8 \\\\ 2 \\end{bmatrix}\\end{aligned}\\]" } ]
